{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3742c8e",
   "metadata": {},
   "source": [
    "## Supervised Machine Learning - Example 02\n",
    "# Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dd6f60",
   "metadata": {},
   "source": [
    "_\"A machine learning technique that assigns a set of predefined categories to open-ended text.\"_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3af5b4",
   "metadata": {},
   "source": [
    "This example uses **Scikit-learn** (http://scikit-learn.org/).\n",
    "\n",
    "**Scikit-learn** is an open-source machine learning library for Python that offers a variety of regression, classification and clustering algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e437e8d7",
   "metadata": {},
   "source": [
    "## Objective:\n",
    "* To automatically flag phishing messages using their content.\n",
    "\n",
    "### Input data:\n",
    "\n",
    "**Origin:**\n",
    "* emails-enron.csv: Regular, non-phishy emails from the Enron email corpus;\n",
    "* emails-phishing.csv: Phishing email corpus.\n",
    "\n",
    "<i>Both files were created using a custom fork of: https://github.com/diegoocampoh/MachineLearningPhishing</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc7fbf7",
   "metadata": {},
   "source": [
    "**Features description:**\n",
    "* ID: (numerical) Extracted e-mail ID from the mbox file;\n",
    "* Content Type: (object) E-mail's [content type](https://en.wikipedia.org/wiki/MIME);\n",
    "* Message: (object) E-mail's payload content;\n",
    "\n",
    "**Label**\n",
    "* Phishy: (boolean) True if from the phishing e-mail corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600d4546",
   "metadata": {},
   "source": [
    "## Summary:\n",
    "\n",
    "1. [Import and Load](#p1)\n",
    "2. [Data Exploration](#p2)\n",
    "3. [Prepare the Data Set](#p3)\n",
    "4. [Train and Test](#p4)\n",
    "5. [Exercise](#p5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3dc483",
   "metadata": {},
   "source": [
    "<a id=\"p1\"></a>\n",
    "## 1. Import and Load\n",
    "\n",
    "The data set is a concatenation of two [CSV](https://en.wikipedia.org/wiki/Comma-separated_values) files created from the [Enron email corpus](https://www.cs.cmu.edu/~enron/) and an available [phishing corpus](http://monkey.org/%7Ejose/wiki/doku.php?id=PhishingCorpus)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066be5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b577baa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(map(pd.read_csv, \n",
    "                   ['data/emails-enron.csv', \n",
    "                    'data/emails-phishing.csv']))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b279c645",
   "metadata": {},
   "source": [
    "<a id=\"p2\"></a>\n",
    "## 2. Data Exploration\n",
    "\n",
    "Initial step in data analysis, where we explore a large data set in an unstructured way to uncover initial patterns, characteristics  and points of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad290123",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926e3043",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8348e4b",
   "metadata": {},
   "source": [
    "### Check for missing values\n",
    "\n",
    "Missing data can reduce the statistical power of a study and can produce biased estimates, leading to invalid conclusions ([Why are missing values a problem?](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3668100/))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ae6e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb365a6c",
   "metadata": {},
   "source": [
    "### Take a quick look at the *Phishy* label column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75439049",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Phishy'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e81c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Phishy'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3be51f",
   "metadata": {},
   "source": [
    "We can see that we have a [balanced dataset](https://medium.com/analytics-vidhya/what-is-balance-and-imbalance-dataset-89e8d7f46bc5): 2000 out of 4000 e-mails (50%) are _Phishy_.\n",
    "\n",
    "This means that any machine learning model we create has to perform **better than 50%** to beat random chance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a727c9",
   "metadata": {},
   "source": [
    "### Check the features' column types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7992b63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f2716a",
   "metadata": {},
   "source": [
    "### Bag of Words and TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1cac19",
   "metadata": {},
   "source": [
    "A **Bag of Words** is a simplified representation used in natural language processing, where a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9466f1",
   "metadata": {},
   "source": [
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f248ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentences\n",
    "s1 = \"In a hole in the ground there lived a hobbit.\"\n",
    "s2 = \"It does not do to leave a live dragon out of your calculations, if you live near him.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4512349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vocabulary\n",
    "vocab = {}\n",
    "i = 1\n",
    "\n",
    "for word in s1.lower().split()+s2.lower().split():\n",
    "    if word in vocab:\n",
    "        continue\n",
    "    else:\n",
    "        vocab[word]=i\n",
    "        i+=1\n",
    "        \n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a576661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty vectors with an index for each word in the vocabulary\n",
    "s1_vector = ['s1']+[0]*len(vocab)\n",
    "\n",
    "# Map the frequencies of each word to the vectors\n",
    "for word in s1.lower().split():\n",
    "    s1_vector[vocab[word]]+=1\n",
    "    \n",
    "print(s1_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f614b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty vectors with an index for each word in the vocabulary\n",
    "s2_vector = ['s2']+[0]*len(vocab)\n",
    "\n",
    "# Map the frequencies of each word to the vectors\n",
    "for word in s2.lower().split():\n",
    "    s2_vector[vocab[word]]+=1\n",
    "    \n",
    "print(s2_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870c046a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectors comparison\n",
    "print(f'{s1_vector}\\n{s2_vector}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655f158b",
   "metadata": {},
   "source": [
    "In the above example, each vector can be considered a **bag of words**. Extending this logic to thousands of entries, we could see that the vocabulary dictionary could grow to hundreds of thousands of words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e043d429",
   "metadata": {},
   "source": [
    "In practice, the bag-of-words model is mainly used as a tool of feature generation. After transforming the text into a \"bag of words\", we can calculate various measures to characterize the text. The most common type of characteristics, or features, calculated from this model is term frequency, namely, the number of times a term appears in the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d2a271",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb11652",
   "metadata": {},
   "source": [
    "**TF-IDF**, short for term frequencyâ€“inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document.\n",
    "\n",
    "It basically considers term frequencies (word occurrences/number of words in the sentence) and inverse document frequency (total number of sentences/the number of sentences that contain the word), which diminishes the weight of terms that occur very frequently in the set and increases the weight of terms that occur rarely."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cc9ecb",
   "metadata": {},
   "source": [
    "### Other interesting NLP definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900bf407",
   "metadata": {},
   "source": [
    "* **Stop words**: frequent words that could be ignored from the vocabulary ('a', 'the', 'and', ...)\n",
    "* **Tokenization**: dividing documents into individual words (frequently uses morphology concepts)\n",
    "* **Word stems**: use of the root of a word/token (e.g.: instead of dragon and dragons, use only dragon)\n",
    "* **Tagging**: adds more dimension to tokens (parts of speech, grammatical dependencies, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babc9132",
   "metadata": {},
   "source": [
    "<a id=\"p3\"></a>\n",
    "## 3. Prepare the Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14f2e1a",
   "metadata": {},
   "source": [
    "### Create Feature and Label sets\n",
    "\n",
    "**Feature** columns are the ones used to predict the **label** columns. This time we want to look at the text.\n",
    "\n",
    "By **convention**, Features are represented as **X** (uppercase) and Lables as **y** (lowercase)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50564020",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df['Message']\n",
    "y = df['Phishy']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db71a0a3",
   "metadata": {},
   "source": [
    "### Split the data into train and test sets\n",
    "\n",
    "Here we'll assign 70% of the data for training and 30% for testing.\n",
    "\n",
    "Also, we are setting a `random_state` seed value to ensure results replicability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9901cc9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24073541",
   "metadata": {},
   "source": [
    "### CountVectorizer\n",
    "Text preprocessing, tokenizing and the ability to filter out stopwords are all included in Scikit-learn's [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html), which builds a dictionary of features and transforms documents to feature vectors, similar to a Bag of Words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbbebf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "\n",
    "X_train_cv = count_vect.fit_transform(X_train)\n",
    "X_train_cv.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182c8596",
   "metadata": {},
   "source": [
    "Our training set is now comprised of 2800 e-mails with <font color=green>**82671**</font> features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ece01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracted features\n",
    "count_vect.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbc25ef",
   "metadata": {},
   "source": [
    "### Tfidf Transformer\n",
    "\n",
    "TF-IDF can then be computed from the count-matrix using Scikit-learn's [TfidfTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07114c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_cv)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b97c183",
   "metadata": {},
   "source": [
    "The `fit_transform()` method performs two operations in this case: it fits an estimator to the data and then transforms our count-matrix (X_train_cv) to a tf-idf representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52f96c8",
   "metadata": {},
   "source": [
    "### TfidfVectorizer\n",
    "Scikit-learn's [TfidVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) combines the CountVectorizer and TfidTransformer steps into one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e755b7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_train_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957abd54",
   "metadata": {},
   "source": [
    "<a id=\"p4\"></a>\n",
    "## 4. Train and Test\n",
    "\n",
    "\n",
    "Since our training set needs to be vectorized before being processed by the classifier, we can use a Scikit-learn's [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html), a class that behaves like a compound classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42af787d",
   "metadata": {},
   "source": [
    "In this example, we are going to use a Linear [Support Vector Classification](https://scikit-learn.org/stable/modules/svm.html) algorithm due to its good performance with sparse input ([LinearSVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd84098c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Pipeline\n",
    "lsvc = Pipeline([\n",
    "                  ('tfidf', TfidfVectorizer()),\n",
    "                  ('clf', LinearSVC()),\n",
    "                 ])\n",
    "\n",
    "# Train\n",
    "lsvc = lsvc.fit(X_train, y_train)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb153ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prediction set\n",
    "predictions = lsvc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0adb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a confusion matrix\n",
    "cm = metrics.confusion_matrix(y_test, predictions, labels=lsvc.classes_)\n",
    "\n",
    "disp = metrics.ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=lsvc.classes_)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4489c7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "print(metrics.classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39e3d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the overall accuracy\n",
    "print(metrics.accuracy_score(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01514f26",
   "metadata": {},
   "source": [
    "<a id=\"p5\"></a>\n",
    "## 5. Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d080249",
   "metadata": {},
   "source": [
    "### 5.1. Feed new data to the model's `predict()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6052c955",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_email = \"Dear user, \\\n",
    "Your e-mail quota is running out. \\\n",
    "Please follow the link below to fix the issue: \\\n",
    "http://www.mailquota.com?/exec/fix\\\n",
    "and update your account information.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab6faf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lsvc.predict([test_email]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3b1d46",
   "metadata": {},
   "source": [
    "### 5.2. Improve your model by using `stop_words`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27896183",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ...\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Pipeline\n",
    "lsvc = Pipeline([\n",
    "                  ('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
    "                  ('clf', LinearSVC()),\n",
    "                 ])\n",
    "\n",
    "# Train\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df92aa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prediction set\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c033ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a confusion matrix\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56fe179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification report\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9caf1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the overall accuracy\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
